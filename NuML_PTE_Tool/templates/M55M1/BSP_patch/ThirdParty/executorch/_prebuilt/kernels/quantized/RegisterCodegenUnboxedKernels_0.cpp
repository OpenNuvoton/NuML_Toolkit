/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

#include <executorch/runtime/core/evalue.h>
#include <executorch/runtime/core/exec_aten/exec_aten.h>
#include <executorch/runtime/core/span.h>
#include <executorch/runtime/kernel/operator_registry.h>
#include <executorch/runtime/platform/profiler.h>
#include "NativeFunctions.h" // Generated Function import headers
// @generated by torchgen/gen_executorch.py from RegisterCodegenUnboxedKernels.cpp

// NOTE [Sharded File]: This file is generated in a sharded fashion to speed up
// incremental rebuilds. See the comment at the top of
// templates/VariableType.cpp for an analogous, in-depth discussion.
//
// Generated by tools/jit/gen_unboxing.py. This file registers all ATen ops into
// JIT op registry instead of c10 dispatcher. JIT op registry only takes boxed
// kernels, so we are calling unboxing functions in UnboxingFunctions.h to cast
// arguments into C++ types (instead of IValue) and delegate to unboxed kernels.
using KernelSpan = ::executorch::runtime::Span<
    const ::executorch::ET_RUNTIME_NAMESPACE::Kernel>;
namespace torch {
namespace executor {
namespace function {
namespace {

static Kernel kernels_to_register[] = {
    
    Kernel(
        "quantized_decomposed::add.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& a = *stack[0];
    	EValue& a_scale = *stack[1];
    	EValue& a_zero_point = *stack[2];
    	EValue& a_quant_min = *stack[3];
    	EValue& a_quant_max = *stack[4];
    	EValue& b = *stack[5];
    	EValue& b_scale = *stack[6];
    	EValue& b_zero_point = *stack[7];
    	EValue& b_quant_min = *stack[8];
    	EValue& b_quant_max = *stack[9];
    	EValue& out_scale = *stack[10];
    	EValue& out_zero_point = *stack[11];
    	EValue& out_quant_min = *stack[12];
    	EValue& out_quant_max = *stack[13];
    	EValue& out = *stack[14];
    	const torch::executor::Tensor & a_base = a.to<torch::executor::Tensor>();
    	double a_scale_base = a_scale.to<double>();
    	int64_t a_zero_point_base = a_zero_point.to<int64_t>();
    	int64_t a_quant_min_base = a_quant_min.to<int64_t>();
    	int64_t a_quant_max_base = a_quant_max.to<int64_t>();
    	const torch::executor::Tensor & b_base = b.to<torch::executor::Tensor>();
    	double b_scale_base = b_scale.to<double>();
    	int64_t b_zero_point_base = b_zero_point.to<int64_t>();
    	int64_t b_quant_min_base = b_quant_min.to<int64_t>();
    	int64_t b_quant_max_base = b_quant_max.to<int64_t>();
    	double out_scale_base = out_scale.to<double>();
    	int64_t out_zero_point_base = out_zero_point.to<int64_t>();
    	int64_t out_quant_min_base = out_quant_min.to<int64_t>();
    	int64_t out_quant_max_base = out_quant_max.to<int64_t>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_add.out");
            EXECUTORCH_SCOPE_PROF("native_call_add.out");
            torch::executor::native::quantized_add_out(context, a_base, a_scale_base, a_zero_point_base, a_quant_min_base, a_quant_max_base, b_base, b_scale_base, b_zero_point_base, b_quant_min_base, b_quant_max_base, out_scale_base, out_zero_point_base, out_quant_min_base, out_quant_max_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[14]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::choose_qparams.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& quant_min = *stack[1];
    	EValue& quant_max = *stack[2];
    	EValue& eps = *stack[3];
    	EValue& dtype = *stack[4];
    	EValue& scale_out = *stack[5];
    	EValue& zero_point_out = *stack[6];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	int64_t quant_min_base = quant_min.to<int64_t>();
    	int64_t quant_max_base = quant_max.to<int64_t>();
    	double eps_base = eps.to<double>();
    	torch::executor::ScalarType dtype_base = dtype.to<torch::executor::ScalarType>();
    	torch::executor::Tensor & scale_out_base = scale_out.to<torch::executor::Tensor>();
    	torch::executor::Tensor & zero_point_out_base = zero_point_out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_choose_qparams.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_choose_qparams.Tensor_out");
            torch::executor::native::choose_qparams_tensor_out(context, input_base, quant_min_base, quant_max_base, eps_base, dtype_base, scale_out_base, zero_point_out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[5]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[6]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::dequantize_per_tensor.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& scale = *stack[1];
    	EValue& zero_point = *stack[2];
    	EValue& quant_min = *stack[3];
    	EValue& quant_max = *stack[4];
    	EValue& dtype = *stack[5];
    	EValue& out_dtype = *stack[6];
    	EValue& out = *stack[7];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	double scale_base = scale.to<double>();
    	int64_t zero_point_base = zero_point.to<int64_t>();
    	int64_t quant_min_base = quant_min.to<int64_t>();
    	int64_t quant_max_base = quant_max.to<int64_t>();
    	torch::executor::ScalarType dtype_base = dtype.to<torch::executor::ScalarType>();
    	
    	    auto out_dtype_opt_out = out_dtype.toOptional<torch::executor::ScalarType>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_dequantize_per_tensor.out");
            EXECUTORCH_SCOPE_PROF("native_call_dequantize_per_tensor.out");
            torch::executor::native::dequantize_per_tensor_out(context, input_base, scale_base, zero_point_base, quant_min_base, quant_max_base, dtype_base, out_dtype_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[7]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::dequantize_per_tensor.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& scale = *stack[1];
    	EValue& zero_point = *stack[2];
    	EValue& quant_min = *stack[3];
    	EValue& quant_max = *stack[4];
    	EValue& dtype = *stack[5];
    	EValue& out_dtype = *stack[6];
    	EValue& out = *stack[7];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & scale_base = scale.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & zero_point_base = zero_point.to<torch::executor::Tensor>();
    	int64_t quant_min_base = quant_min.to<int64_t>();
    	int64_t quant_max_base = quant_max.to<int64_t>();
    	torch::executor::ScalarType dtype_base = dtype.to<torch::executor::ScalarType>();
    	
    	    auto out_dtype_opt_out = out_dtype.toOptional<torch::executor::ScalarType>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_dequantize_per_tensor.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_dequantize_per_tensor.Tensor_out");
            torch::executor::native::dequantize_per_tensor_tensor_args_out(context, input_base, scale_base, zero_point_base, quant_min_base, quant_max_base, dtype_base, out_dtype_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[7]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::quantize_per_channel.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& scales = *stack[1];
    	EValue& zero_points = *stack[2];
    	EValue& axis = *stack[3];
    	EValue& quant_min = *stack[4];
    	EValue& quant_max = *stack[5];
    	EValue& dtype = *stack[6];
    	EValue& out = *stack[7];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & scales_base = scales.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & zero_points_base = zero_points.to<torch::executor::Tensor>();
    	int64_t axis_base = axis.to<int64_t>();
    	int64_t quant_min_base = quant_min.to<int64_t>();
    	int64_t quant_max_base = quant_max.to<int64_t>();
    	torch::executor::ScalarType dtype_base = dtype.to<torch::executor::ScalarType>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_quantize_per_channel.out");
            EXECUTORCH_SCOPE_PROF("native_call_quantize_per_channel.out");
            torch::executor::native::quantize_per_channel_out(context, input_base, scales_base, zero_points_base, axis_base, quant_min_base, quant_max_base, dtype_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[7]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::dequantize_per_channel.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& scales = *stack[1];
    	EValue& zero_points = *stack[2];
    	EValue& axis = *stack[3];
    	EValue& quant_min = *stack[4];
    	EValue& quant_max = *stack[5];
    	EValue& dtype = *stack[6];
    	EValue& out_dtype = *stack[7];
    	EValue& out = *stack[8];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & scales_base = scales.to<torch::executor::Tensor>();
    	
    	    auto zero_points_opt_out = zero_points.toOptional<torch::executor::Tensor>();
    	            
    	int64_t axis_base = axis.to<int64_t>();
    	int64_t quant_min_base = quant_min.to<int64_t>();
    	int64_t quant_max_base = quant_max.to<int64_t>();
    	torch::executor::ScalarType dtype_base = dtype.to<torch::executor::ScalarType>();
    	
    	    auto out_dtype_opt_out = out_dtype.toOptional<torch::executor::ScalarType>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_dequantize_per_channel.out");
            EXECUTORCH_SCOPE_PROF("native_call_dequantize_per_channel.out");
            torch::executor::native::dequantize_per_channel_out(context, input_base, scales_base, zero_points_opt_out, axis_base, quant_min_base, quant_max_base, dtype_base, out_dtype_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[8]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::embedding_byte.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& weight = *stack[0];
    	EValue& weight_scales = *stack[1];
    	EValue& weight_zero_points = *stack[2];
    	EValue& weight_quant_min = *stack[3];
    	EValue& weight_quant_max = *stack[4];
    	EValue& indices = *stack[5];
    	EValue& out = *stack[6];
    	const torch::executor::Tensor & weight_base = weight.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & weight_scales_base = weight_scales.to<torch::executor::Tensor>();
    	
    	    auto weight_zero_points_opt_out = weight_zero_points.toOptional<torch::executor::Tensor>();
    	            
    	int64_t weight_quant_min_base = weight_quant_min.to<int64_t>();
    	int64_t weight_quant_max_base = weight_quant_max.to<int64_t>();
    	const torch::executor::Tensor & indices_base = indices.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_embedding_byte.out");
            EXECUTORCH_SCOPE_PROF("native_call_embedding_byte.out");
            torch::executor::native::quantized_embedding_byte_out(context, weight_base, weight_scales_base, weight_zero_points_opt_out, weight_quant_min_base, weight_quant_max_base, indices_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[6]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::embedding_byte.dtype_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& weight = *stack[0];
    	EValue& weight_scales = *stack[1];
    	EValue& weight_zero_points = *stack[2];
    	EValue& weight_quant_min = *stack[3];
    	EValue& weight_quant_max = *stack[4];
    	EValue& indices = *stack[5];
    	EValue& dtype = *stack[6];
    	EValue& out = *stack[7];
    	const torch::executor::Tensor & weight_base = weight.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & weight_scales_base = weight_scales.to<torch::executor::Tensor>();
    	
    	    auto weight_zero_points_opt_out = weight_zero_points.toOptional<torch::executor::Tensor>();
    	            
    	int64_t weight_quant_min_base = weight_quant_min.to<int64_t>();
    	int64_t weight_quant_max_base = weight_quant_max.to<int64_t>();
    	const torch::executor::Tensor & indices_base = indices.to<torch::executor::Tensor>();
    	
    	    auto dtype_opt_out = dtype.toOptional<torch::executor::ScalarType>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_embedding_byte.dtype_out");
            EXECUTORCH_SCOPE_PROF("native_call_embedding_byte.dtype_out");
            torch::executor::native::quantized_embedding_byte_dtype_out(context, weight_base, weight_scales_base, weight_zero_points_opt_out, weight_quant_min_base, weight_quant_max_base, indices_base, dtype_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[7]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::embedding_2bit.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& weight = *stack[0];
    	EValue& weight_scales = *stack[1];
    	EValue& weight_zero_points = *stack[2];
    	EValue& weight_quant_min = *stack[3];
    	EValue& weight_quant_max = *stack[4];
    	EValue& indices = *stack[5];
    	EValue& out = *stack[6];
    	const torch::executor::Tensor & weight_base = weight.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & weight_scales_base = weight_scales.to<torch::executor::Tensor>();
    	
    	    auto weight_zero_points_opt_out = weight_zero_points.toOptional<torch::executor::Tensor>();
    	            
    	int64_t weight_quant_min_base = weight_quant_min.to<int64_t>();
    	int64_t weight_quant_max_base = weight_quant_max.to<int64_t>();
    	const torch::executor::Tensor & indices_base = indices.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_embedding_2bit.out");
            EXECUTORCH_SCOPE_PROF("native_call_embedding_2bit.out");
            torch::executor::native::quantized_embedding_2bit_out(context, weight_base, weight_scales_base, weight_zero_points_opt_out, weight_quant_min_base, weight_quant_max_base, indices_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[6]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::embedding_2bit.dtype_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& weight = *stack[0];
    	EValue& weight_scales = *stack[1];
    	EValue& weight_zero_points = *stack[2];
    	EValue& weight_quant_min = *stack[3];
    	EValue& weight_quant_max = *stack[4];
    	EValue& indices = *stack[5];
    	EValue& dtype = *stack[6];
    	EValue& out = *stack[7];
    	const torch::executor::Tensor & weight_base = weight.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & weight_scales_base = weight_scales.to<torch::executor::Tensor>();
    	
    	    auto weight_zero_points_opt_out = weight_zero_points.toOptional<torch::executor::Tensor>();
    	            
    	int64_t weight_quant_min_base = weight_quant_min.to<int64_t>();
    	int64_t weight_quant_max_base = weight_quant_max.to<int64_t>();
    	const torch::executor::Tensor & indices_base = indices.to<torch::executor::Tensor>();
    	
    	    auto dtype_opt_out = dtype.toOptional<torch::executor::ScalarType>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_embedding_2bit.dtype_out");
            EXECUTORCH_SCOPE_PROF("native_call_embedding_2bit.dtype_out");
            torch::executor::native::quantized_embedding_2bit_dtype_out(context, weight_base, weight_scales_base, weight_zero_points_opt_out, weight_quant_min_base, weight_quant_max_base, indices_base, dtype_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[7]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::embedding_4bit.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& weight = *stack[0];
    	EValue& weight_scales = *stack[1];
    	EValue& weight_zero_points = *stack[2];
    	EValue& weight_quant_min = *stack[3];
    	EValue& weight_quant_max = *stack[4];
    	EValue& indices = *stack[5];
    	EValue& out = *stack[6];
    	const torch::executor::Tensor & weight_base = weight.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & weight_scales_base = weight_scales.to<torch::executor::Tensor>();
    	
    	    auto weight_zero_points_opt_out = weight_zero_points.toOptional<torch::executor::Tensor>();
    	            
    	int64_t weight_quant_min_base = weight_quant_min.to<int64_t>();
    	int64_t weight_quant_max_base = weight_quant_max.to<int64_t>();
    	const torch::executor::Tensor & indices_base = indices.to<torch::executor::Tensor>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_embedding_4bit.out");
            EXECUTORCH_SCOPE_PROF("native_call_embedding_4bit.out");
            torch::executor::native::quantized_embedding_4bit_out(context, weight_base, weight_scales_base, weight_zero_points_opt_out, weight_quant_min_base, weight_quant_max_base, indices_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[6]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::embedding_4bit.dtype_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& weight = *stack[0];
    	EValue& weight_scales = *stack[1];
    	EValue& weight_zero_points = *stack[2];
    	EValue& weight_quant_min = *stack[3];
    	EValue& weight_quant_max = *stack[4];
    	EValue& indices = *stack[5];
    	EValue& dtype = *stack[6];
    	EValue& out = *stack[7];
    	const torch::executor::Tensor & weight_base = weight.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & weight_scales_base = weight_scales.to<torch::executor::Tensor>();
    	
    	    auto weight_zero_points_opt_out = weight_zero_points.toOptional<torch::executor::Tensor>();
    	            
    	int64_t weight_quant_min_base = weight_quant_min.to<int64_t>();
    	int64_t weight_quant_max_base = weight_quant_max.to<int64_t>();
    	const torch::executor::Tensor & indices_base = indices.to<torch::executor::Tensor>();
    	
    	    auto dtype_opt_out = dtype.toOptional<torch::executor::ScalarType>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_embedding_4bit.dtype_out");
            EXECUTORCH_SCOPE_PROF("native_call_embedding_4bit.dtype_out");
            torch::executor::native::quantized_embedding_4bit_dtype_out(context, weight_base, weight_scales_base, weight_zero_points_opt_out, weight_quant_min_base, weight_quant_max_base, indices_base, dtype_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[7]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::mixed_mm.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& weight = *stack[1];
    	EValue& weight_scales = *stack[2];
    	EValue& weight_zero_points = *stack[3];
    	EValue& out = *stack[4];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & weight_base = weight.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & weight_scales_base = weight_scales.to<torch::executor::Tensor>();
    	
    	    auto weight_zero_points_opt_out = weight_zero_points.toOptional<torch::executor::Tensor>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_mixed_mm.out");
            EXECUTORCH_SCOPE_PROF("native_call_mixed_mm.out");
            torch::executor::native::quantized_mixed_mm_out(context, input_base, weight_base, weight_scales_base, weight_zero_points_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[4]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::mixed_linear.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& weight = *stack[1];
    	EValue& weight_scales = *stack[2];
    	EValue& weight_zero_points = *stack[3];
    	EValue& dtype = *stack[4];
    	EValue& out = *stack[5];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & weight_base = weight.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & weight_scales_base = weight_scales.to<torch::executor::Tensor>();
    	
    	    auto weight_zero_points_opt_out = weight_zero_points.toOptional<torch::executor::Tensor>();
    	            
    	
    	    auto dtype_opt_out = dtype.toOptional<torch::executor::ScalarType>();
    	            
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_mixed_linear.out");
            EXECUTORCH_SCOPE_PROF("native_call_mixed_linear.out");
            torch::executor::native::quantized_mixed_linear_out(context, input_base, weight_base, weight_scales_base, weight_zero_points_opt_out, dtype_opt_out, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[5]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::quantize_per_tensor.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& scale = *stack[1];
    	EValue& zero_point = *stack[2];
    	EValue& quant_min = *stack[3];
    	EValue& quant_max = *stack[4];
    	EValue& dtype = *stack[5];
    	EValue& out = *stack[6];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	double scale_base = scale.to<double>();
    	int64_t zero_point_base = zero_point.to<int64_t>();
    	int64_t quant_min_base = quant_min.to<int64_t>();
    	int64_t quant_max_base = quant_max.to<int64_t>();
    	torch::executor::ScalarType dtype_base = dtype.to<torch::executor::ScalarType>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_quantize_per_tensor.out");
            EXECUTORCH_SCOPE_PROF("native_call_quantize_per_tensor.out");
            torch::executor::native::quantize_per_tensor_out(context, input_base, scale_base, zero_point_base, quant_min_base, quant_max_base, dtype_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[6]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::quantize_per_tensor.Tensor_out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& scale = *stack[1];
    	EValue& zero_point = *stack[2];
    	EValue& quant_min = *stack[3];
    	EValue& quant_max = *stack[4];
    	EValue& dtype = *stack[5];
    	EValue& out = *stack[6];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & scale_base = scale.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & zero_point_base = zero_point.to<torch::executor::Tensor>();
    	int64_t quant_min_base = quant_min.to<int64_t>();
    	int64_t quant_max_base = quant_max.to<int64_t>();
    	torch::executor::ScalarType dtype_base = dtype.to<torch::executor::ScalarType>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_quantize_per_tensor.Tensor_out");
            EXECUTORCH_SCOPE_PROF("native_call_quantize_per_tensor.Tensor_out");
            torch::executor::native::quantize_per_tensor_tensor_args_out(context, input_base, scale_base, zero_point_base, quant_min_base, quant_max_base, dtype_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[6]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::choose_qparams_per_token_asymmetric.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& dtype = *stack[1];
    	EValue& scale_out = *stack[2];
    	EValue& zero_point_out = *stack[3];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	torch::executor::ScalarType dtype_base = dtype.to<torch::executor::ScalarType>();
    	torch::executor::Tensor & scale_out_base = scale_out.to<torch::executor::Tensor>();
    	torch::executor::Tensor & zero_point_out_base = zero_point_out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_choose_qparams_per_token_asymmetric.out");
            EXECUTORCH_SCOPE_PROF("native_call_choose_qparams_per_token_asymmetric.out");
            torch::executor::native::choose_qparams_per_token_asymmetric_out(context, input_base, dtype_base, scale_out_base, zero_point_out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[2]);
    internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[3]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::quantize_per_token.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& scales = *stack[1];
    	EValue& zero_points = *stack[2];
    	EValue& quant_min = *stack[3];
    	EValue& quant_max = *stack[4];
    	EValue& dtype = *stack[5];
    	EValue& out = *stack[6];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & scales_base = scales.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & zero_points_base = zero_points.to<torch::executor::Tensor>();
    	int64_t quant_min_base = quant_min.to<int64_t>();
    	int64_t quant_max_base = quant_max.to<int64_t>();
    	torch::executor::ScalarType dtype_base = dtype.to<torch::executor::ScalarType>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_quantize_per_token.out");
            EXECUTORCH_SCOPE_PROF("native_call_quantize_per_token.out");
            torch::executor::native::quantize_per_token_out(context, input_base, scales_base, zero_points_base, quant_min_base, quant_max_base, dtype_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[6]);
    
            
    
        }
    ),
    
    Kernel(
        "quantized_decomposed::dequantize_per_token.out",
        [](torch::executor::KernelRuntimeContext & context, EValue** stack) {
            EValue& input = *stack[0];
    	EValue& scales = *stack[1];
    	EValue& zero_points = *stack[2];
    	EValue& quant_min = *stack[3];
    	EValue& quant_max = *stack[4];
    	EValue& dtype = *stack[5];
    	EValue& output_dtype = *stack[6];
    	EValue& out = *stack[7];
    	const torch::executor::Tensor & input_base = input.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & scales_base = scales.to<torch::executor::Tensor>();
    	const torch::executor::Tensor & zero_points_base = zero_points.to<torch::executor::Tensor>();
    	int64_t quant_min_base = quant_min.to<int64_t>();
    	int64_t quant_max_base = quant_max.to<int64_t>();
    	torch::executor::ScalarType dtype_base = dtype.to<torch::executor::ScalarType>();
    	torch::executor::ScalarType output_dtype_base = output_dtype.to<torch::executor::ScalarType>();
    	torch::executor::Tensor & out_base = out.to<torch::executor::Tensor>();
    
    
            internal::EventTracerProfileOpScope event_tracer_op_scope(context.internal_event_tracer(), "native_call_dequantize_per_token.out");
            EXECUTORCH_SCOPE_PROF("native_call_dequantize_per_token.out");
            torch::executor::native::dequantize_per_token_out(context, input_base, scales_base, zero_points_base, quant_min_base, quant_max_base, dtype_base, output_dtype_base, out_base);
            internal::event_tracer_log_evalue(context.internal_event_tracer(), *stack[7]);
    
            
    
        }
    ), // Generated kernels
};

// Explicitly convert to Span, so that the API can take an empty C array of
// Kernels.
static KernelSpan kernel_span(
    kernels_to_register,
    kernels_to_register + sizeof(kernels_to_register) / sizeof(Kernel));

// Return value not used. Keep the static variable assignment to register
// kernels in static initialization time.
static auto success_with_kernel_reg = register_kernels(kernel_span);
} // namespace
} // namespace function
} // namespace executor
} // namespace torch
